{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deepfake Detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch.nn as nn\n",
    "import cv2\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import timm\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random_seed = 42\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed_all(random_seed)\n",
    "\n",
    "# Set paths\n",
    "data_folder = \"Model_10k\"\n",
    "\n",
    "# Function to perform random rotation with seed\n",
    "def random_rotation_with_seed(image, degrees):\n",
    "    # Set the random seed for the rotation\n",
    "    random.seed(random_seed)\n",
    "    return F.rotate(image, degrees)\n",
    "\n",
    "# Preprocessing transforms with data augmentation\n",
    "preprocess_transform = transforms.Compose([\n",
    "    transforms.Lambda(lambda x: random_rotation_with_seed(x, 10)),  # Set the seed for random rotation\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Define custom dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_paths, class_labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.class_labels = class_labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        image = Image.open(image_path).convert(\"RGB\")  # Open image and convert to RGB mode\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        label = self.class_labels[idx]\n",
    "        return image, label\n",
    "\n",
    "# Define a custom binary loss function\n",
    "class BinaryLossFunction(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BinaryLossFunction, self).__init__()\n",
    "\n",
    "    def forward(self, outputs, labels):\n",
    "        loss = nn.BCEWithLogitsLoss()(outputs, labels)\n",
    "        return loss\n",
    "\n",
    "def load_data(data_folder, dataset_name, limit=None):\n",
    "    images, all_images = [], []\n",
    "    class_labels, classes_label, all_labels = [], [], []\n",
    "    subfolders = [\"ADM\", \"DDPM\", \"IDDPM\", \"LDM\", \"PNDM\"]\n",
    "    count = 0\n",
    "    for subfolder in subfolders:\n",
    "        print(\"Folder : \",data_folder, dataset_name, subfolder)\n",
    "        if dataset_name == \"test\" and count == 0:\n",
    "          folder_path = os.path.join(data_folder, dataset_name)\n",
    "          # print(folder_path)\n",
    "          for image_type in os.listdir(folder_path):  # Iterate through subfolders\n",
    "              subfolder_path = os.path.join(folder_path, image_type)  # Get the path of the current subfolder\n",
    "              # print(subfolder_path, folder_path, image_type)\n",
    "              # print(\"Processing subfolder:\", subfolder_path, subfolder, image_type)\n",
    "              class_images, class_labels = load_class_data(subfolder_path, image_type, limit)  # Load images from the current subfolder with a limit\n",
    "              # print(\"Class images:\", class_images)\n",
    "              images.extend(class_images)\n",
    "              classes_label.extend(class_labels)\n",
    "              count += 1\n",
    "        elif dataset_name == \"train\" or dataset_name == \"val\":\n",
    "          folder_path = os.path.join(data_folder, dataset_name, subfolder)  # Updated folder_path\n",
    "          for image_type in os.listdir(folder_path):  # Iterate through subfolders\n",
    "              subfolder_path = os.path.join(folder_path, image_type)  # Get the path of the current subfolder\n",
    "              # print(subfolder_path)\n",
    "              # print(\"Processing subfolder:\", subfolder_path, subfolder, image_type)\n",
    "              class_images, class_labels = load_class_data(folder_path, image_type, limit)  # Load images from the current subfolder with a limit\n",
    "              # print(\"Class images:\", class_images)\n",
    "              images.extend(class_images)\n",
    "              classes_label.extend(class_labels)\n",
    "          label_counts = Counter(classes_label)\n",
    "          print(\"Label Counts:\", label_counts)\n",
    "\n",
    "        all_images = images\n",
    "        all_labels = classes_label\n",
    "    return all_images, all_labels\n",
    "\n",
    "def load_class_data(folder, class_label, limit=None):\n",
    "    if \"test\" in folder and not \"real\" in class_label:\n",
    "        images = [os.path.join(folder, filename) for filename in os.listdir(os.path.join(folder)) if filename.endswith(\".png\")]\n",
    "        class_labels = [1] * len(images)\n",
    "    else:\n",
    "        images = [os.path.join(folder, class_label, filename) for filename in os.listdir(os.path.join(folder, class_label)) if filename.endswith(\".png\")]\n",
    "        class_labels = [1] * len(images) if class_label == \"1_fake\" else [0] * len(images)\n",
    "    # Shuffle the data if limit is not None\n",
    "    if limit:\n",
    "        data = list(zip(images, class_labels))\n",
    "        random.shuffle(data)\n",
    "        images, class_labels = zip(*data)\n",
    "        images = images[:limit]\n",
    "        class_labels = class_labels[:limit]\n",
    "    print(folder, class_labels[0], len(class_labels))\n",
    "    return images, class_labels\n",
    "\n",
    "# Load images and class labels for training set with a limit of 10000 samples\n",
    "train_images, train_labels = load_data(data_folder, \"train\", limit=2000)\n",
    "\n",
    "# Load images and class labels for validation set with a limit of 20000 samples\n",
    "val_images, val_labels = load_data(data_folder, \"val\", limit=500)\n",
    "\n",
    "# Create train and validation datasets\n",
    "train_dataset = CustomDataset(train_image_paths, train_class_labels, transform=preprocess_transform)\n",
    "val_dataset = CustomDataset(val_image_paths, val_class_labels, transform=preprocess_transform)\n",
    "\n",
    "# Create train and validation data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Count the occurrences of each class label\n",
    "train_label_counts = Counter(train_labels)\n",
    "val_label_counts = Counter(val_labels)\n",
    "print(\"Train Label Counts:\", train_label_counts)\n",
    "print(\"Validation Label Counts:\", val_label_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your own custom model or use a pretrained model\n",
    "model = timm.create_model('xception', pretrained=True)\n",
    "num_features = model.num_features\n",
    "num_classes = 1\n",
    "model.fc = nn.Linear(num_features, num_classes)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = BinaryLossFunction()  # Use the custom binary loss function\n",
    "optimizer = Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)  # Add weight decay for L2 regularization\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=3, verbose=True)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "best_val_accuracy = 0.0\n",
    "best_model_path = \"best_model_xception_All.pth\"  # Path to save the best model\n",
    "\n",
    "patience = 5  # Number of epochs to wait for improvement\n",
    "counter = 0  # Counter to track the number of epochs without improvement\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "\n",
    "    progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}', leave=False)\n",
    "\n",
    "    for images, labels in progress_bar:\n",
    "        images = images.to(device)\n",
    "        labels = labels.float().to(device)  # Move labels to device and convert to float\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs.view(-1), labels)  # Flatten the output to match the target size\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        sig_outputs = torch.sigmoid(outputs)\n",
    "        predicted = (sig_outputs >= 0.5).float()  # Convert sigmoid outputs to binary predictions\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels.view_as(predicted)).sum().item()  # Compare predictions and labels\n",
    "        progress_bar.set_postfix({'Loss': loss.item(), 'Accuracy': (predicted == labels).sum().item() / labels.size(0)})\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    train_accuracy = correct_train / total_train\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "\n",
    "    # Evaluation on the validation set\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.float().to(device)  # Move labels to device and convert to float\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs.view(-1), labels)  # Flatten the output to match the target size\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            sig_outputs = torch.sigmoid(outputs)\n",
    "            predicted = (sig_outputs >= 0.5).float()  # Convert sigmoid outputs to binary predictions\n",
    "            total_val += labels.size(0)\n",
    "            correct_val += (predicted == labels.view_as(predicted)).sum().item()  # Compare predictions and labels\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_accuracy = correct_val / total_val\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "    # Learning rate scheduler step\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # Check for early stopping based on both loss and accuracy\n",
    "    if val_loss < best_val_loss or val_accuracy > best_val_accuracy:\n",
    "        if best_val_loss > val_loss:\n",
    "            best_val_loss = val_loss\n",
    "        if best_val_accuracy < val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "        counter = 0\n",
    "\n",
    "        # Save the best model\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(\"Validation loss and accuracy did not improve for {} epochs. Early stopping.\".format(patience))\n",
    "            break\n",
    "\n",
    "# Loading the best model for test evaluation\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "\n",
    "# Plotting the loss and accuracy curves\n",
    "epochs = range(1, len(train_losses) + 1)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, train_losses, label='Train')\n",
    "plt.plot(epochs, val_losses, label='Validation')\n",
    "plt.title('Training and Validation and Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, train_accuracies, label='Train')\n",
    "plt.plot(epochs, val_accuracies, label='Validation')\n",
    "plt.title('Training and Validation and Test Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qAT8JG9xzlqS",
    "outputId": "b060629a-ed58-44a1-92e3-a964201ee70f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/timm/models/_factory.py:114: UserWarning: Mapping deprecated model name xception to current legacy_xception.\n",
      "  model = create_fn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Label Counts: Counter({0: 2000, 1: 2000})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [12:12<00:00,  2.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Real Images: 0.9800\n",
      "Accuracy for Fake Images: 0.9945\n",
      "Overall Accuracy: 0.9872\n",
      "Average Precision Score: 0.9990\n",
      "AUROC: 0.9992\n",
      "Pd@FAR (FAR = 5%): 0.9990\n",
      "Pd@FAR (FAR = 1%): 0.9915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, average_precision_score\n",
    "\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random_seed = 42\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed_all(random_seed)\n",
    "\n",
    "# Set paths\n",
    "data_folder = \"Model_10k\"\n",
    "\n",
    "# Preprocessing transforms with data augmentation\n",
    "preprocess_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Define custom test dataset class\n",
    "class CustomTestDataset(Dataset):\n",
    "    def __init__(self, image_paths, class_labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.class_labels = class_labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        image = Image.open(image_path).convert(\"RGB\")  # Open image and convert to RGB mode\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        label = self.class_labels[idx]\n",
    "        return image, label\n",
    "\n",
    "\n",
    "# Define a custom binary loss function\n",
    "class BinaryLossFunction(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BinaryLossFunction, self).__init__()\n",
    "\n",
    "    def forward(self, outputs, labels):\n",
    "        loss = nn.BCEWithLogitsLoss()(outputs, labels)\n",
    "        return loss\n",
    "\n",
    "def load_data(folder, class_label):\n",
    "    images = [os.path.join(folder, class_label, filename) for filename in os.listdir(os.path.join(folder, class_label)) if filename.endswith(\".png\")]\n",
    "    class_labels = [1] * len(images) if class_label == \"1_fake\" else [0] * len(images)\n",
    "    return images, class_labels\n",
    "\n",
    "def shuffle_data(image_paths, class_labels):\n",
    "    data = list(zip(image_paths, class_labels))\n",
    "    random.shuffle(data)\n",
    "    image_paths, class_labels = zip(*data)\n",
    "    return image_paths, class_labels\n",
    "\n",
    "def limit_data(image_paths, class_labels, limit):\n",
    "    return image_paths[:limit], class_labels[:limit]\n",
    "\n",
    "# Load the saved model\n",
    "model = timm.create_model('xception', pretrained=False)\n",
    "num_features = model.num_features\n",
    "num_classes = 1\n",
    "model.fc = nn.Linear(num_features, num_classes)\n",
    "model.load_state_dict(torch.load(\"best_model_xception_All.pth\"))\n",
    "model.eval()\n",
    "\n",
    "# Move the model to the GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Load new test images and labels\n",
    "new_test_real_images, new_test_real_labels = load_data(os.path.join(data_folder, \"test_new\", \"PNDM\"), \"0_real\")\n",
    "new_test_fake_images, new_test_fake_labels = load_data(os.path.join(data_folder, \"test_new\", \"PNDM\"), \"1_fake\")\n",
    "new_test_image_paths = new_test_real_images + new_test_fake_images\n",
    "new_test_class_labels = new_test_real_labels + new_test_fake_labels\n",
    "\n",
    "test_label_counts = Counter(new_test_class_labels)\n",
    "print(\"Test Label Counts:\", test_label_counts)\n",
    "\n",
    "\n",
    "# Create a custom test dataset for new test images\n",
    "new_test_dataset = CustomTestDataset(new_test_image_paths, new_test_class_labels, transform=preprocess_transform)\n",
    "\n",
    "# Create a data loader for new test images\n",
    "new_test_loader = DataLoader(new_test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Create lists to store the predicted probabilities and true labels for real and fake images\n",
    "all_predicted_probabilities = []\n",
    "all_true_labels = []\n",
    "\n",
    "# Evaluation on new test set\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(new_test_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.float().to(device)  # Move labels to device and convert to float\n",
    "\n",
    "        outputs = model(images)\n",
    "        sig_outputs = torch.sigmoid(outputs)\n",
    "\n",
    "        all_predicted_probabilities.extend(sig_outputs.cpu().numpy())\n",
    "        all_true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Convert lists to NumPy arrays\n",
    "all_true_labels = np.array(all_true_labels)\n",
    "all_predicted_probabilities = np.array(all_predicted_probabilities)\n",
    "\n",
    "# Separate real and fake images based on class labels\n",
    "real_mask = np.array(new_test_class_labels) == 0\n",
    "fake_mask = np.array(new_test_class_labels) == 1\n",
    "\n",
    "# Assuming the variables `all_true_labels` and `all_predicted_probabilities` contain the true labels and predicted probabilities for the new test images, respectively.\n",
    "\n",
    "# Convert lists to NumPy arrays\n",
    "y_true = np.array(all_true_labels)\n",
    "y_pred = np.array(all_predicted_probabilities)\n",
    "\n",
    "# Calculate accuracy for real and fake images separately\n",
    "r_acc = accuracy_score(y_true[y_true == 0], y_pred[y_true == 0] > 0.5)\n",
    "f_acc = accuracy_score(y_true[y_true == 1], y_pred[y_true == 1] > 0.5)\n",
    "\n",
    "# Calculate overall accuracy\n",
    "acc = accuracy_score(y_true, y_pred > 0.5)\n",
    "\n",
    "# Calculate average precision score\n",
    "ap = average_precision_score(y_true, y_pred)\n",
    "\n",
    "auroc = roc_auc_score(y_true, y_pred)\n",
    "\n",
    "\n",
    "# Calculate the probability of detection at a fixed false alarm rate (Pd@FAR) at FARs of 5% and 1%\n",
    "far_5_percent_threshold = np.percentile(y_pred[y_true == 0], 95)\n",
    "far_1_percent_threshold = np.percentile(y_pred[y_true == 0], 99)\n",
    "\n",
    "pd_at_far_5_percent = np.mean(y_pred[y_true == 1] >= far_5_percent_threshold)\n",
    "pd_at_far_1_percent = np.mean(y_pred[y_true == 1] >= far_1_percent_threshold)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Accuracy for Real Images: {r_acc:.4f}\")\n",
    "print(f\"Accuracy for Fake Images: {f_acc:.4f}\")\n",
    "print(f\"Overall Accuracy: {acc:.4f}\")\n",
    "print(f\"Average Precision Score: {ap:.4f}\")\n",
    "print(f\"AUROC: {auroc:.4f}\")\n",
    "print(f\"Pd@FAR (FAR = 5%): {pd_at_far_5_percent:.4f}\")\n",
    "print(f\"Pd@FAR (FAR = 1%): {pd_at_far_1_percent:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
